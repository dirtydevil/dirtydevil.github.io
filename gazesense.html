<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Kunal Gupta || dirtydevil</title>

    <!-- Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <link href="css/dirtydevil.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/freelancer.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <!-- <link href="http://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css"> -->
    <!-- <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css"> -->
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700,300,400' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="http://kunalgupta.in">Kunal Gupta</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="page-scroll">
                        <a href="http://kunalgupta.in#page-top">About</a>
                    </li>
                    <li class="page-scroll">
                        <a href="http://kunalgupta.in#portfolio">Portfolio</a>
                    </li>
                    <!-- <li class="page-scroll">
                        <a href="#about">About</a>
                    </li> -->

                    <li class="page-scroll">
                        <a href="kunal_resume.pdf" target="_blank">Resume</a>
                    </li>
                    <li class="page-scroll">
                        <a href="http://www.flickr.com/photos/dirtydevil-kunal/" target="_blank">WeirdoGraphy</a>
                    </li>
                    <li class="page-scroll">
                        <a href="http://dirtydebiandevil.wordpress.com/" target="_blank">Blog</a>
                    </li>
                    <li class="page-scroll">
                        <a href="http://kunalgupta.in#contact">Contact</a>
                    </li>

                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>



        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header>
      <!-- <div class="modal-content">
      <div class="close-modal" data-dismiss="modal">
          <div class="lr">
              <div class="rl">
              </div>
          </div>
      </div> -->


      <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Gaze-Sense</h2>
                            <hr class="star-primary">
                               <p style="float: left; font-size: 9pt; text-align: center; width: 100%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/gsense/prototype.jpg" style="width: 100%"></p>
                             </br>

                            <p style="text-align: justify;">
                            In the projects CSense and CoSense, we focused on the sharing of emotional states with the remote person in order to augment empathy in both.
                            There are many examples of how remote collaboration may help a person to perform the real world tasks more efficiently. For example, a surgeon performing an operation in an absense of a specialist surgeon, so what if the technology could be used to allow the specialist to participate remotely and instruct the operating room staff and surgeon in order to complete the task successfully.
                            However, participating remotely is not the same as being there in person. In particular, it may be difficult for a remote person to know the exact focus of attention of the local worker (the person performing the actions). Now imagine if the specialist knows the exact focus of attention of the operating staff, he could have been more specific with the instructions and felt more conected and co-present.

                            </p></br>

                            <p style="text-align: justify;">

                              The goal of our research project Gaze-Sense is to explore if sharing the focus of attention using eye-tracking information of a local worker can help in feeling the remote instructor (helper) more connected and co-present in a remote assistance scenario.


                              <!--CoSense is a prototype wearable interface that shares a user’s first person video and their current emotional state with a remote user in order to create a shared emotional experience.
                              -->
                            </p></br>

                            <p style="text-align: justify;">
                              The research questions for this research project are:

                              <ol style="text-align: left;">
                                <li>Can sharing of the Focus of Attention (FoA) of a Worker to a Helper, using eye-tracking, make an impact on the connectedness and co-presence of the remote collaboration?
                                <li>Can virtual co-presence be increased by combining the Worker’s Focus of Attention and the Helper’s Annotation in athe shared visual space?
                                <li>Can sharing of Worker’s Focus of Attention and the Helper’s annotations increase task performance?
                                </ol>
                          </p></br>

                            <p style="text-align: justify;">
                              For this system, the main components are:
                                  <ol style="text-align: left;">
                                    <li>Head mounted eye tracking system (ET) , </li></br>
                                    <li>Head mounted camera (HWC) that streams the POV view of the user, </li></br>
                                    <li>Head mounted display (HMD), </li></br>
                                    <li>Remote viewing software that allows a remote expert to see and annotate on the local worker’s view. </li></br>
                                  </ol>
                            </p></br>


                            <p style="text-align: justify;">
                              We made a rough block diagram for the whole idea.
                            </p></br>

                            <img src="img/portfolio/gsense/system.png" class="img-responsive img-centered" alt=""></br>

                            <p style="text-align: justify;">
                             For HMD, we chose Brother AirScouter, a high quality, optical see-through monocular 800 x 600 resolution display with a field of view of 22.4 degrees. In order to align the display with the user's sight line, it has an adjustable mounting mechanism.
                           </p>

                           <p style="text-align: justify;">
                            We developed a custom made eye tracker using Microsoft Lifecam HD 5000 and custom 3D printed enclosure pointing at user's eye. Now to superimpose eye gaze information on the POV real world, we used Logitech webcam C920 on the system facing outwards as the HWC.
                          </p>

                           <p style="float: left; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/gsense/bs.jpg" style="width: 100%"></p>
                           <p style="float: right; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/gsense/etscreen.png" style="width: 100%"></p>



                           <p style="text-align: justify;">
                            In the software part, there were two main components. First was the eye tracking application that was tracking the eye pupil and overlaying the gaze information as a marker on the real world video feed captured from HWC. We modified the open source eye tracking software developed at Pupil Labs that uses computer vision to track eye pupil at 30 frames per second and map it over the video from HWC.  The second was the annotation application that allowed the remote helper to move a mouse pointer over the remote video view of the local worker and shows the result in the display of the HMD. It was a screen cast of remote desktop over the local worker's HMD along with the mouse pointer annotation developed in .NET language.
                          </p>


                          <img src="img/portfolio/gsense/ethmd1.png" class="img-responsive img-centered" alt=""></br>


                          <p style="text-align: justify;">
                           A 2 by 2 Experiment was designed to validate the hypothesis. In the experiment, we used a remote assistance scenario where a local worker will be given a task and the remote helper who will have experience of performing the task will be assisting the local worker. For the tasks we designed 4 LEGO duplo structures with equal pieces and equal difficulty level. The local worker had to follow remote helper's instruction and perform the task. There was a time that had to be reset before it reaches 0 seconds in order to provide multiple focus points at a time that could relate to an operation theatre scenario.
                         </p>


                         <p style="text-align: justify;">
                           For the experiment, the conditions that we had were:

                           <ol style="text-align: left;">
                             <li><i>No Cue (None)</i>: only audio and live video were shared, </li></br>
                             <li><i>Pointer Cue (P)</i>: A virtual pointer was used by remote helper to aid local worker.When the remote helper clicks on the shared video on his display, a green virtual pointer appears at the clicked position, and is also shown on the local worker’s HMD. The pointer is provided as an additional cue to the basic audio and video cues in the NONE condition. , </li></br>
                             <li><i>Eye Tracker Cue (E)</i>: An eye gaze marker (red circle) is displayed on the remote helper’s monitor to show the local worker’s focus of attention. The eye gaze was provided as an additional cue to the basic audio cues in the NONE condition. , </li></br>
                             <li><i>Both Pointer and Eye-Tracker Cues (BOTH)</i>: Both eye-tracker and pointer cues were provided in addition to the cues provided in NONE condition. </li></br>
                           </ol>
                     </p></br>


                             <!-- <p style="float: left; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/1.png" style="width: 100%"></p>
                             <p style="float: right; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/2.png" style="width: 100%"></p>

                             <p style="float: left; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/3.jpg" style="width: 100%"></p>
                             <p style="float: right; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/4.png" style="width: 100%"></p> -->


                             <p style="text-align: justify;">
                             The results from the experiment can be seen in the <a href= "http://ir.canterbury.ac.nz/handle/10092/11389" target="_blank">Master's thesis available in the University of Canterbury's online repository.</a>

                          </p>



                             <p style="text-align: justify;">
                             <b>Project Supervisor: </b> Dr. Christoph Bartneck </br>
                             <b>Project Co-Supervisor: </b> Prof. Mark Billinghurst and Dr. Gun Lee </br>
                             </p>


                            <a href="http://kunalgupta.in/#portfolio" class="btn btn-default" role="button"><i class="fa fa-arrow-left"></i> Portfolio</a>
                        </div>
                    </div>
                </div>
            </div>

    </header>


    <!-- Footer -->
    <footer class="text-center">


        <div class="footer-below">
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        Copyright &copy; dirtydevil 2015
                    </div>
                </div>
            </div>
        </div>
    </footer>


    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) -->
    <div class="scroll-top page-scroll visible-xs visble-sm visible-md visible-lg">
        <a class="btn btn-primary" href="#page-top">
            <i class="fa fa-chevron-up"></i>
        </a>
    </div>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="http://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="js/classie.js"></script>
    <script src="js/cbpAnimatedHeader.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/freelancer.js"></script>

    <!--Processing JS -->
    <script type="text/javascript" src="processing.js"></script>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-84072659-1', 'auto');
  ga('send', 'pageview');

</script>

</body>

</html>
