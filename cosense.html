<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Kunal Gupta || dirtydevil</title>

    <!-- Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <link href="css/dirtydevil.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/freelancer.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <!-- <link href="http://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css"> -->
    <!-- <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css"> -->
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700,300,400' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" class="index">

<script type="text/javascript">
    window._pt_lt = new Date().getTime();
    window._pt_sp_2 = [];
    _pt_sp_2.push('setAccount,41aaf8d6');
    var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
    (function() {
        var atag = document.createElement('script'); atag.type = 'text/javascript'; atag.async = true;
        atag.src = _protocol + 'cjs.ptengine.com/pta_en.js';
        var stag = document.createElement('script'); stag.type = 'text/javascript'; stag.async = true;
        stag.src = _protocol + 'cjs.ptengine.com/pts.js';
        var s = document.getElementsByTagName('script')[0]; 
        s.parentNode.insertBefore(atag, s); s.parentNode.insertBefore(stag, s);
    })();
</script>
                        

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="http://kunalgupta.in">Kunal Gupta</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="page-scroll">
                        <a href="http://kunalgupta.in#page-top">About</a>
                    </li>
                    <li class="page-scroll">
                        <a href="http://kunalgupta.in#portfolio">Portfolio</a>
                    </li>
                    <!-- <li class="page-scroll">
                        <a href="#about">About</a>
                    </li> -->

                    <li class="page-scroll">
                        <a href="kunal_resume.pdf" target="_blank">Resume</a>
                    </li>
                    <li class="page-scroll">
                        <a href="http://www.flickr.com/photos/dirtydevil-kunal/" target="_blank">WeirdoGraphy</a>
                    </li>
                    <li class="page-scroll">
                        <a href="http://dirtydebiandevil.wordpress.com/" target="_blank">Blog</a>
                    </li>
                    <li class="page-scroll">
                        <a href="http://kunalgupta.in#contact">Contact</a>
                    </li>

                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>



        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header>
      <!-- <div class="modal-content">
      <div class="close-modal" data-dismiss="modal">
          <div class="lr">
              <div class="rl">
              </div>
          </div>
      </div> -->


      <div class="container">
          <div class="row">
              <div class="col-lg-8 col-lg-offset-2">
                  <div class="modal-body">
                      <h2>CoSense: Creating Shared Emotional Experiences</h2>
                      <hr class="star-primary">
                       <div class='embed-container'>
                       <iframe src="https://www.youtube.com/embed/Z7o9e1i7fnE" frameborder='2px' allowfullscreen></iframe>
                       </div>
                       </br>

                      <p style="text-align: justify;">
                        Wearable devices such as Google Glass have cameras and microphones in them that enable video and audio to be streamed to a remote person. This allows the remote person to hear and
      see with the ears and eyes of the Google Glass user. However there has been relatively little research on using wearable technology like this to enable people to share feelings as well.
                      </p></br>

                      <p style="text-align: justify;">
                        The goal of our research project CoSense is to explore if sharing physiological sensor data in real time between people can be used to increase shared emotional experiences and create more empathy. This is a part of broader aim to develop wearable systems that will enable a user to share what they are seeing, hearing and feeling with another person.
                        <!--CoSense is a prototype wearable interface that shares a user’s first person video and their current emotional state with a remote user in order to create a shared emotional experience.
                        -->
                      </p></br>

                      <p style="text-align: justify;">
                        The problem statement for this research project is: “How can wearable devices be used to share emotional experiences between users and so create a deeper sense of empathy and understanding?"

                      </p></br>


                      <p style="text-align: justify;">
                        In the CSense research, passive monitoring of emotions was mainly explored, where a person continuously monitors their emotional levels during their everyday activities and make the data available to a close friend or family member. For example a daughter may check on her elderly mother’s heart rate from time to time to make sure that she is doing okay.
                      </p></br>

                      <p style="text-align: justify;">
                        However, in this research we will restrict out scope to active collaboration of emotional state, where a person is engaged in a short period of activity and wants to have a remote person share the experience with them. For example, going for a roller coaster ride for a few minutes. We wanted to use physiological sensors to capture what a user is feeling, and wearable cameras/microphones to record what they are seeing and hearing. Then we  want to be able to transmit these feelings, sights and sounds to a remote user to create a shared emotional experience.
                      </p></br>

                      <p style="text-align: justify;">
                        For this system, the main components are:
                            <ol style="text-align: left;">
                              <li>Wearable computer such as Google Glass that will stream video and audio of the user, </li></br>
                              <li>Sensor system to compute emotions using physiological data, </li></br>
                              <li> Desktop interface for the remote user to view the images and emotional cues being sent from the wearable user. </li></br>
                            </ol>
                      </p></br>


                      <p style="text-align: justify;">
                        We made a rough block diagram for the whole idea.
                      </p></br>

                      <img src="img/portfolio/cosense/BD.png" class="img-responsive img-centered" alt=""></br>

                      <p style="text-align: justify;">
                       Prototype included a combination of sensors like GSR, Blood Oxymeter and Hear rate sensor to recognise emotions. Google glass was used to capture local user’s POV and send to remote user (laptop for prototype) using spydroid -ipcamera.
                     </p>

                     <p style="float: left; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/eh.png" style="width: 100%"></p>
                     <p style="float: right; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/eh1.png" style="width: 100%"></p>

                     <p style="text-align: justify;">
                       One of the research challenge is how to represent the emotional cues in the interface. We explored different ways of showing the user’s emotions using following cues:

                       <ol style="text-align: left;">
                         <li>Raw sensor data/graphs, </li></br>
                         <li>Emotion Labels, </li></br>
                         <li>Image Graphic Overlay. </li></br>
                       </ol>
                 </p></br>

                 <p style="float: left; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/graph.png" style="width: 100%"></p>
                 <p style="float: right; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/tint.png" style="width: 100%"></p>

                 <!-- <p style="float: left; font-size: 9pt; text-align: center; width: 32%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/sketch3.png" style="width: 100%"></p> -->

               </br>

                 <p style="text-align: justify;">
                   To get an initial idea of how to put these cues together, I sketched and since we had only two factors for the experiment i.e. graphs and colored ovelay, I had to design it using these.
                </p></br>


                 <p style="float: left; font-size: 9pt; text-align: center; width: 32%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/sketch1.png" style="width: 100%"></p>
                 <p style="float: right; font-size: 9pt; text-align: center; width: 32%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/sketch2.png" style="width: 100%"></p>

                 <p style="float: left; font-size: 9pt; text-align: center; width: 32%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/sketch3.png" style="width: 100%"></p>
                 <!-- <p style="float: right; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/eh1.png" style="width: 100%"></p> -->

                 <p style="text-align: justify;">
                   The final interface looked was designed and developed.
                </p></br>

                <!-- <div class='embed-container'> -->
                 <img src="img/portfolio/cosense/final.png" class="img-responsive img-centered" alt=""></br>


                     <p style="text-align: justify;">

                     </p>



                     <p style="text-align: justify;">
                       A user evaluation was conducted to explore which interface cues best helped a remote user to understand what the local user was feeling.
                     </p>
                     <p style="text-align: justify;">
                          We evaluated from 4 different interfaces, which were </br>

                          <!-- <ol style="text-align: left;">
                              <li>Just video sharing, </li></br>

                              <li>Video along with raw data from Blood oxygen level, heart rate, skin conductance, skin resistance and skin voltage in form of graphs, </li> </br>

                              <li>Video with colour overlay based on particular emotions, and</li></br>

                              <li>Coloured overlay video along with all the raw data graphs as mentioned in 2nd interface.</li>

                          </ol> -->

                          <p style="text-align: justify;">C1. No Cues: Just video,</p>
                          <p style="text-align: justify;">C2. Emotion Cue: emotional label, heart rate and video,</p>
                          <p style="text-align: justify;">C3. Raw Graphs: video with raw data graphs, and </p>
                          <p style="text-align: justify;">C4. All Cues: Video, raw data graphs, heart rate and emotion tag.</p>
                       </p>

                       <img src="img/portfolio/cosense/exp.png" class="img-responsive img-centered" alt=""></br>

                       <!-- <p style="float: left; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/1.png" style="width: 100%"></p>
                       <p style="float: right; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/2.png" style="width: 100%"></p>

                       <p style="float: left; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/3.jpg" style="width: 100%"></p>
                       <p style="float: right; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/4.png" style="width: 100%"></p> -->


                       <p style="text-align: justify;">
                       From the ranking questionnaire, we found that C2(Emotion Cues) was  significantly better than C1 (No Cues), C3 (Raw Graph Cues) and C4 ( All cues) for questions like How strongly do you feel the emotion? and How well do you think you understood how your partner was feeling?. Whereas C1 (No Cues) was significantly better than C2, C3, C4 for question like How easy was it to understand the interface?

                     </p></br>
                       <p style="text-align: justify;">
                       For complete explanation of the results see the Work in Progress paper published at CHI 2015, Seoul, South Korea.
                       </p>

                       <p style="text-align: justify;">
                      From these results, we observed that the system we developed created an awareness of the Sender’s emotional state in the Receiver. These tests suggested that the Receiver could perceive a deeper understanding of the Sender’s emotional state if they were provided with some emotional representation in a visual form along with audio and video of the Sender’s environment.
                       </p>

                       <p style="text-align:justify;">
                         Condition 2 (live video color tinted with user’s heart rate and an emotional state label), was felt to be more helpful by Receivers than the interfaces showing the raw sensor data (Condition 3) and even Condition 4 in which both Conditions 2 and 3 were mixed.
                       </p>


                       <p style="float: left; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/quotation1.png" style="width: 100%"></p>
                       <p style="float: right; font-size: 9pt; text-align: center; width: 49%; margin-right: 1%; margin-bottom: 0.5em;"><img src="img/portfolio/cosense/quotation2.png" style="width: 100%"></p>

                       <p style="text-align: justify;">
                       <b>Project Supervisor: </b> Prof. Mark Billinghurst </br>
                       <b>Project Members: </b> </br> Sudhanshu Ayyagari, </br>
                       Kunal Gupta and </br>
                       Matthew Tait.
                       </p>

                      <a href="http://kunalgupta.in/#portfolio" class="btn btn-default" role="button"><i class="fa fa-arrow-left"></i> Portfolio</a>
                      <!-- <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-arrow-left"></i> Back</button> -->
                  </div>
              </div>
          </div>
      </div>

    </header>


    <!-- Footer -->
    <footer class="text-center">


        <div class="footer-below">
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        Copyright &copy; dirtydevil 2015
                    </div>
                </div>
            </div>
        </div>
    </footer>


    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) -->
    <div class="scroll-top page-scroll visible-xs visble-sm visible-md visible-lg">
        <a class="btn btn-primary" href="#page-top">
            <i class="fa fa-chevron-up"></i>
        </a>
    </div>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="http://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="js/classie.js"></script>
    <script src="js/cbpAnimatedHeader.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/freelancer.js"></script>

    <!--Processing JS -->
    <script type="text/javascript" src="processing.js"></script>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-84072659-1', 'auto');
  ga('send', 'pageview');

</script>

</body>

</html>
